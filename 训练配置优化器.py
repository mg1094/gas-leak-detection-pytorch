"""
ç‡ƒæ°”æ³„æ¼æ£€æµ‹ç³»ç»Ÿ - è®­ç»ƒé…ç½®ä¼˜åŒ–å™¨
Training Configuration Optimizer for Gas Leak Detection System
"""

import os
import pandas as pd
import numpy as np
import json
from datetime import datetime, timedelta
import argparse

class TrainingConfigOptimizer:
    """è®­ç»ƒé…ç½®ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        # åŸºå‡†é…ç½®
        self.base_config = {
            'lstm_units': 50,
            'lstm_layers': 2,
            'sequence_length': 60,
            'features_count': 3,
            'batch_size': 32,
            'epochs': 100,
            'learning_rate': 0.001
        }
        
        # æ¨¡å‹å¤æ‚åº¦
        self.model_params = self._calculate_model_params()
        
    def _calculate_model_params(self):
        """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
        config = self.base_config
        
        # LSTMå‚æ•°é‡è®¡ç®—
        # å…¬å¼: 4 * (input_size + hidden_size + 1) * hidden_size
        lstm_params = 0
        
        # ç¬¬ä¸€å±‚LSTM
        first_layer = 4 * (config['features_count'] + config['lstm_units'] + 1) * config['lstm_units']
        lstm_params += first_layer
        
        # åç»­LSTMå±‚
        for i in range(1, config['lstm_layers']):
            layer_params = 4 * (config['lstm_units'] + config['lstm_units'] + 1) * config['lstm_units']
            lstm_params += layer_params
        
        # å…¨è¿æ¥å±‚å‚æ•° (ç®€åŒ–ä¼°ç®—)
        fc_params = config['lstm_units'] * 32 + 32 * 16 + 16 * 8
        
        total_params = lstm_params + fc_params
        
        return {
            'lstm_params': lstm_params,
            'fc_params': fc_params,
            'total_params': total_params
        }
        
    def analyze_data(self, data_path=None):
        """åˆ†æè®­ç»ƒæ•°æ®"""
        if data_path is None:
            # åˆ†ææµ‹è¯•ç”¨ä¾‹æ•°æ®
            data_info = self._analyze_test_cases()
        else:
            # åˆ†ææŒ‡å®šæ•°æ®æ–‡ä»¶
            data_info = self._analyze_data_file(data_path)
            
        return data_info
        
    def _analyze_test_cases(self):
        """åˆ†ææµ‹è¯•ç”¨ä¾‹æ•°æ®"""
        test_cases_dir = 'data/test_cases'
        
        if not os.path.exists(test_cases_dir):
            return None
            
        total_samples = 0
        total_leak_samples = 0
        total_normal_samples = 0
        data_files = []
        
        for filename in os.listdir(test_cases_dir):
            if filename.endswith('.csv') and filename != 'test_summary.json':
                file_path = os.path.join(test_cases_dir, filename)
                try:
                    df = pd.read_csv(file_path, index_col=0, parse_dates=True)
                    
                    # è®¡ç®—åºåˆ—æ ·æœ¬æ•°é‡
                    sequence_samples = max(0, len(df) - self.base_config['sequence_length'] + 1)
                    leak_samples = df['is_leak'].sum() if 'is_leak' in df.columns else 0
                    normal_samples = sequence_samples - leak_samples
                    
                    total_samples += sequence_samples
                    total_leak_samples += leak_samples
                    total_normal_samples += normal_samples
                    
                    data_files.append({
                        'filename': filename,
                        'total_points': len(df),
                        'sequence_samples': sequence_samples,
                        'leak_samples': leak_samples,
                        'normal_samples': normal_samples,
                        'time_span': (df.index[-1] - df.index[0]).total_seconds() / 3600  # å°æ—¶
                    })
                    
                except Exception as e:
                    print(f"âš ï¸ åˆ†ææ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
                    
        return {
            'total_sequence_samples': total_samples,
            'total_leak_samples': total_leak_samples,
            'total_normal_samples': total_normal_samples,
            'leak_ratio': total_leak_samples / total_samples if total_samples > 0 else 0,
            'data_files': data_files,
            'estimated_time_span': sum(f['time_span'] for f in data_files)
        }
        
    def _analyze_data_file(self, data_path):
        """åˆ†æå•ä¸ªæ•°æ®æ–‡ä»¶"""
        try:
            df = pd.read_csv(data_path, index_col=0, parse_dates=True)
            
            # è®¡ç®—åºåˆ—æ ·æœ¬æ•°é‡
            sequence_samples = max(0, len(df) - self.base_config['sequence_length'] + 1)
            leak_samples = df['is_leak'].sum() if 'is_leak' in df.columns else 0
            normal_samples = sequence_samples - leak_samples
            
            time_span = (df.index[-1] - df.index[0]).total_seconds() / 3600  # å°æ—¶
            
            return {
                'total_sequence_samples': sequence_samples,
                'total_leak_samples': leak_samples,
                'total_normal_samples': normal_samples,
                'leak_ratio': leak_samples / sequence_samples if sequence_samples > 0 else 0,
                'estimated_time_span': time_span,
                'data_files': [{
                    'filename': os.path.basename(data_path),
                    'total_points': len(df),
                    'sequence_samples': sequence_samples,
                    'leak_samples': leak_samples,
                    'normal_samples': normal_samples,
                    'time_span': time_span
                }]
            }
            
        except Exception as e:
            print(f"âŒ åˆ†ææ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
            return None
            
    def recommend_config(self, data_info):
        """æ ¹æ®æ•°æ®é‡æ¨èè®­ç»ƒé…ç½®"""
        if data_info is None:
            print("âŒ æ— æ³•è·å–æ•°æ®ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self.base_config.copy()
            
        total_samples = data_info['total_sequence_samples']
        leak_ratio = data_info['leak_ratio']
        time_span = data_info['estimated_time_span']
        
        print(f"ğŸ“Š æ•°æ®åˆ†æç»“æœ:")
        print(f"   æ€»åºåˆ—æ ·æœ¬æ•°: {total_samples:,}")
        print(f"   æ³„æ¼æ ·æœ¬æ•°: {data_info['total_leak_samples']:,}")
        print(f"   æ­£å¸¸æ ·æœ¬æ•°: {data_info['total_normal_samples']:,}")
        print(f"   æ³„æ¼æ¯”ä¾‹: {leak_ratio:.1%}")
        print(f"   æ•°æ®æ—¶é—´è·¨åº¦: {time_span:.1f} å°æ—¶")
        
        # æ ¹æ®æ•°æ®é‡çº§åˆ«ç¡®å®šé…ç½®
        if total_samples < 100_000:
            level = "å°æ•°æ®é›†"
            config = self._get_small_dataset_config()
        elif total_samples < 1_000_000:
            level = "ä¸­ç­‰æ•°æ®é›†"
            config = self._get_medium_dataset_config()
        elif total_samples < 10_000_000:
            level = "å¤§æ•°æ®é›†"
            config = self._get_large_dataset_config()
        else:
            level = "è¶…å¤§æ•°æ®é›†"
            config = self._get_xlarge_dataset_config()
            
        # æ ¹æ®æ•°æ®ä¸å¹³è¡¡è°ƒæ•´é…ç½®
        config = self._adjust_for_imbalance(config, leak_ratio)
        
        # æ ¹æ®æ—¶é—´è·¨åº¦è°ƒæ•´é…ç½®
        config = self._adjust_for_timespan(config, time_span)
        
        print(f"\nğŸ¯ æ¨èé…ç½®çº§åˆ«: {level}")
        
        return config
        
    def _get_small_dataset_config(self):
        """å°æ•°æ®é›†é…ç½®"""
        config = self.base_config.copy()
        config.update({
            'epochs': 150,
            'batch_size': 64,
            'learning_rate': 0.01,
            'early_stopping_patience': 20,
            'lr_decay_patience': 10,
            'training_mode': 'å¿«é€ŸéªŒè¯'
        })
        return config
        
    def _get_medium_dataset_config(self):
        """ä¸­ç­‰æ•°æ®é›†é…ç½®"""
        config = self.base_config.copy()
        config.update({
            'epochs': 120,
            'batch_size': 32,
            'learning_rate': 0.001,
            'early_stopping_patience': 15,
            'lr_decay_patience': 8,
            'training_mode': 'æ ‡å‡†è®­ç»ƒ'
        })
        return config
        
    def _get_large_dataset_config(self):
        """å¤§æ•°æ®é›†é…ç½®"""
        config = self.base_config.copy()
        config.update({
            'epochs': 80,
            'batch_size': 16,
            'learning_rate': 0.0005,
            'early_stopping_patience': 12,
            'lr_decay_patience': 6,
            'training_mode': 'é«˜æ•ˆè®­ç»ƒ'
        })
        return config
        
    def _get_xlarge_dataset_config(self):
        """è¶…å¤§æ•°æ®é›†é…ç½®"""
        config = self.base_config.copy()
        config.update({
            'epochs': 50,
            'batch_size': 8,
            'learning_rate': 0.0001,
            'early_stopping_patience': 10,
            'lr_decay_patience': 5,
            'training_mode': 'åˆ†å¸ƒå¼è®­ç»ƒ'
        })
        return config
        
    def _adjust_for_imbalance(self, config, leak_ratio):
        """æ ¹æ®æ•°æ®ä¸å¹³è¡¡è°ƒæ•´é…ç½®"""
        if leak_ratio < 0.05:  # æ³„æ¼æ ·æœ¬å°‘äº5%
            config['class_weight'] = 'balanced'
            config['epochs'] = int(config['epochs'] * 1.2)  # å¢åŠ è®­ç»ƒè½®æ•°
            print("   âš ï¸ æ£€æµ‹åˆ°æ•°æ®ä¸å¹³è¡¡ï¼Œå¯ç”¨ç±»åˆ«æƒé‡å¹³è¡¡")
        elif leak_ratio > 0.3:  # æ³„æ¼æ ·æœ¬è¶…è¿‡30%
            config['epochs'] = int(config['epochs'] * 0.8)  # å‡å°‘è®­ç»ƒè½®æ•°
            print("   â„¹ï¸ æ³„æ¼æ ·æœ¬è¾ƒå¤šï¼Œé€‚å½“å‡å°‘è®­ç»ƒè½®æ•°")
            
        return config
        
    def _adjust_for_timespan(self, config, time_span):
        """æ ¹æ®æ—¶é—´è·¨åº¦è°ƒæ•´é…ç½®"""
        if time_span < 24:  # å°‘äº1å¤©
            config['validation_split'] = 0.3  # å¢åŠ éªŒè¯é›†æ¯”ä¾‹
            print("   âš ï¸ æ•°æ®æ—¶é—´è·¨åº¦è¾ƒçŸ­ï¼Œå¢åŠ éªŒè¯é›†æ¯”ä¾‹")
        elif time_span > 720:  # è¶…è¿‡30å¤©
            config['validation_split'] = 0.15  # å‡å°‘éªŒè¯é›†æ¯”ä¾‹
            config['shuffle'] = True  # å¯ç”¨æ•°æ®æ‰“ä¹±
            print("   âœ… æ•°æ®æ—¶é—´è·¨åº¦å……è¶³ï¼Œå¯ç”¨æ•°æ®æ‰“ä¹±")
        else:
            config['validation_split'] = 0.2  # æ ‡å‡†éªŒè¯é›†æ¯”ä¾‹
            
        return config
        
    def estimate_training_time(self, config, data_info):
        """ä¼°ç®—è®­ç»ƒæ—¶é—´"""
        if data_info is None:
            return "æ— æ³•ä¼°ç®—"
            
        total_samples = data_info['total_sequence_samples']
        epochs = config['epochs']
        batch_size = config['batch_size']
        
        # åŸºäºç»éªŒçš„æ—¶é—´ä¼°ç®—ï¼ˆç§’ï¼‰
        # å‡è®¾æ¯ä¸ªæ ·æœ¬å¤„ç†æ—¶é—´çº¦ä¸º0.001ç§’ï¼ˆCPUï¼‰æˆ–0.0001ç§’ï¼ˆGPUï¼‰
        time_per_sample = 0.001  # CPUæ—¶é—´
        
        # è®¡ç®—æ¯ä¸ªepochçš„æ—¶é—´
        batches_per_epoch = np.ceil(total_samples / batch_size)
        time_per_epoch = batches_per_epoch * batch_size * time_per_sample
        
        # æ€»è®­ç»ƒæ—¶é—´
        total_time_seconds = time_per_epoch * epochs
        
        # è½¬æ¢ä¸ºå°æ—¶
        total_time_hours = total_time_seconds / 3600
        
        if total_time_hours < 1:
            return f"{total_time_seconds/60:.0f} åˆ†é’Ÿ"
        elif total_time_hours < 24:
            return f"{total_time_hours:.1f} å°æ—¶"
        else:
            return f"{total_time_hours/24:.1f} å¤©"
            
    def generate_config_file(self, config, output_path='optimized_config.json'):
        """ç”Ÿæˆé…ç½®æ–‡ä»¶"""
        config_with_meta = {
            'generation_time': datetime.now().isoformat(),
            'model_params': self.model_params,
            'training_config': config,
            'usage_instructions': {
                'description': 'ç‡ƒæ°”æ³„æ¼æ£€æµ‹ç³»ç»Ÿä¼˜åŒ–è®­ç»ƒé…ç½®',
                'how_to_use': [
                    'å°†æ­¤é…ç½®æ–‡ä»¶å¯¼å…¥åˆ°è®­ç»ƒè„šæœ¬ä¸­',
                    'æ ¹æ®å®é™…ç¡¬ä»¶èµ„æºè°ƒæ•´batch_size',
                    'ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å˜åŒ–',
                    'ä½¿ç”¨æ—©åœæœºåˆ¶é¿å…è¿‡æ‹Ÿåˆ'
                ]
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(config_with_meta, f, indent=2, ensure_ascii=False)
            
        print(f"ğŸ“„ é…ç½®æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_path}")
        
    def print_recommendations(self, config, data_info):
        """æ‰“å°è®­ç»ƒå»ºè®®"""
        print("\nğŸ¯ è®­ç»ƒé…ç½®å»ºè®®:")
        print("=" * 50)
        
        # åŸºç¡€é…ç½®
        print("ğŸ“‹ åŸºç¡€å‚æ•°:")
        print(f"   è®­ç»ƒè½®æ•° (epochs): {config['epochs']}")
        print(f"   æ‰¹æ¬¡å¤§å° (batch_size): {config['batch_size']}")
        print(f"   å­¦ä¹ ç‡ (learning_rate): {config['learning_rate']}")
        print(f"   è®­ç»ƒæ¨¡å¼: {config.get('training_mode', 'æ ‡å‡†è®­ç»ƒ')}")
        
        # é«˜çº§é…ç½®
        print(f"\nâš™ï¸ é«˜çº§å‚æ•°:")
        print(f"   æ—©åœè€å¿ƒå€¼: {config.get('early_stopping_patience', 15)}")
        print(f"   å­¦ä¹ ç‡è¡°å‡è€å¿ƒå€¼: {config.get('lr_decay_patience', 8)}")
        print(f"   éªŒè¯é›†æ¯”ä¾‹: {config.get('validation_split', 0.2)}")
        
        if 'class_weight' in config:
            print(f"   ç±»åˆ«æƒé‡: {config['class_weight']}")
            
        # æ—¶é—´ä¼°ç®—
        estimated_time = self.estimate_training_time(config, data_info)
        print(f"\nâ±ï¸ é¢„ä¼°è®­ç»ƒæ—¶é—´: {estimated_time}")
        
        # ç¡¬ä»¶å»ºè®®
        print(f"\nğŸ’» ç¡¬ä»¶å»ºè®®:")
        if data_info and data_info['total_sequence_samples'] > 1_000_000:
            print("   - æ¨èä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒ")
            print("   - å†…å­˜éœ€æ±‚: 16GB+")
            print("   - å­˜å‚¨ç©ºé—´: 10GB+")
        else:
            print("   - CPUè®­ç»ƒå³å¯æ»¡è¶³éœ€æ±‚")
            print("   - å†…å­˜éœ€æ±‚: 8GB+")
            print("   - å­˜å‚¨ç©ºé—´: 5GB+")
            
        # ç›‘æ§å»ºè®®
        print(f"\nğŸ“Š è®­ç»ƒç›‘æ§å»ºè®®:")
        print("   - ç›‘æ§è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿")
        print("   - å…³æ³¨å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡å˜åŒ–")
        print("   - ä½¿ç”¨TensorBoardå¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹")
        print("   - å®šæœŸä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç‡ƒæ°”æ³„æ¼æ£€æµ‹ç³»ç»Ÿè®­ç»ƒé…ç½®ä¼˜åŒ–å™¨')
    parser.add_argument('--data-path', type=str, help='è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default='optimized_config.json', 
                       help='è¾“å‡ºé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--analyze-only', action='store_true', 
                       help='ä»…åˆ†ææ•°æ®ï¼Œä¸ç”Ÿæˆé…ç½®')
    
    args = parser.parse_args()
    
    print("ğŸ”§ ç‡ƒæ°”æ³„æ¼æ£€æµ‹ç³»ç»Ÿ - è®­ç»ƒé…ç½®ä¼˜åŒ–å™¨")
    print("=" * 60)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = TrainingConfigOptimizer()
    
    # åˆ†ææ•°æ®
    print("ğŸ” åˆ†æè®­ç»ƒæ•°æ®...")
    data_info = optimizer.analyze_data(args.data_path)
    
    if data_info is None:
        print("âŒ æ— æ³•åˆ†ææ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„")
        return
        
    if args.analyze_only:
        print("\nâœ… æ•°æ®åˆ†æå®Œæˆ")
        return
        
    # æ¨èé…ç½®
    print("\nğŸ¯ ç”Ÿæˆè®­ç»ƒé…ç½®å»ºè®®...")
    config = optimizer.recommend_config(data_info)
    
    # æ‰“å°å»ºè®®
    optimizer.print_recommendations(config, data_info)
    
    # ç”Ÿæˆé…ç½®æ–‡ä»¶
    print(f"\nğŸ’¾ ä¿å­˜é…ç½®æ–‡ä»¶...")
    optimizer.generate_config_file(config, args.output)
    
    print(f"\nâœ… é…ç½®ä¼˜åŒ–å®Œæˆï¼")
    print(f"ğŸ“ é…ç½®æ–‡ä»¶: {args.output}")
    print(f"ğŸ’¡ ä½¿ç”¨æ–¹æ³•: å°†é…ç½®å¯¼å…¥åˆ°è®­ç»ƒè„šæœ¬ä¸­")

if __name__ == '__main__':
    main()