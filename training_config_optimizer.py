"""
ç‡ƒæ°”æ³„æ¼æ£€æµ‹ç³»ç»Ÿ - è®­ç»ƒé…ç½®ä¼˜åŒ–å™¨
Training Configuration Optimizer for Gas Leak Detection System
"""

import os
import pandas as pd
import numpy as np
import json
from datetime import datetime, timedelta
import argparse

class TrainingConfigOptimizer:
    """è®­ç»ƒé…ç½®ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        # åŸºå‡†é…ç½®
        self.base_config = {
            'lstm_units': 50,
            'lstm_layers': 2,
            'sequence_length': 60,
            'features_count': 3,
            'batch_size': 32,
            'epochs': 100,
            'learning_rate': 0.001
        }
        
        # æ¨¡å‹å¤æ‚åº¦
        self.model_params = self._calculate_model_params()
        
    def _calculate_model_params(self):
        """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
        config = self.base_config
        
        # LSTMå‚æ•°é‡è®¡ç®—
        # å…¬å¼: 4 * (input_size + hidden_size + 1) * hidden_size
        lstm_params = 0
        
        # ç¬¬ä¸€å±‚LSTM
        first_layer = 4 * (config['features_count'] + config['lstm_units'] + 1) * config['lstm_units']
        lstm_params += first_layer
        
        # åç»­LSTMå±‚
        for i in range(1, config['lstm_layers']):
            layer_params = 4 * (config['lstm_units'] + config['lstm_units'] + 1) * config['lstm_units']
            lstm_params += layer_params
        
        # å…¨è¿æ¥å±‚å‚æ•° (ç®€åŒ–ä¼°ç®—)
        fc_params = config['lstm_units'] * 32 + 32 * 16 + 16 * 8
        
        total_params = lstm_params + fc_params
        
        return {
            'lstm_params': lstm_params,
            'fc_params': fc_params,
            'total_params': total_params
        }
        
    def analyze_data(self, data_path=None):
        """åˆ†æè®­ç»ƒæ•°æ®"""
        if data_path is None:
            # åˆ†ææµ‹è¯•ç”¨ä¾‹æ•°æ®
            data_info = self._analyze_test_cases()
        else:
            # åˆ†ææŒ‡å®šæ•°æ®æ–‡ä»¶
            data_info = self._analyze_data_file(data_path)
            
        return data_info
        
    def _analyze_test_cases(self):
        """åˆ†ææµ‹è¯•ç”¨ä¾‹æ•°æ®"""
        test_cases_dir = 'data/test_cases'
        
        if not os.path.exists(test_cases_dir):
            return None
            
        total_samples = 0
        total_leak_samples = 0
        total_normal_samples = 0
        data_files = []
        
        for filename in os.listdir(test_cases_dir):
            if filename.endswith('.csv') and filename != 'test_summary.json':
                file_path = os.path.join(test_cases_dir, filename)
                try:
                    df = pd.read_csv(file_path, index_col=0, parse_dates=True)
                    
                    # è®¡ç®—åºåˆ—æ ·æœ¬æ•°é‡
                    sequence_samples = max(0, len(df) - self.base_config['sequence_length'] + 1)
                    leak_samples = df['is_leak'].sum() if 'is_leak' in df.columns else 0
                    normal_samples = sequence_samples - leak_samples
                    
                    total_samples += sequence_samples
                    total_leak_samples += leak_samples
                    total_normal_samples += normal_samples
                    
                    data_files.append({
                        'filename': filename,
                        'total_points': len(df),
                        'sequence_samples': sequence_samples,
                        'leak_samples': leak_samples,
                        'normal_samples': normal_samples,
                        'time_span': (df.index[-1] - df.index[0]).total_seconds() / 3600  # å°æ—¶
                    })
                    
                except Exception as e:
                    print(f"âš ï¸ åˆ†ææ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
                    
        return {
            'total_sequence_samples': total_samples,
            'total_leak_samples': total_leak_samples,
            'total_normal_samples': total_normal_samples,
            'leak_ratio': total_leak_samples / total_samples if total_samples > 0 else 0,
            'data_files': data_files,
            'estimated_time_span': sum(f['time_span'] for f in data_files)
        }

    def recommend_config(self, data_info):
        """æ ¹æ®æ•°æ®é‡æ¨èè®­ç»ƒé…ç½®"""
        if data_info is None:
            print("âŒ æ— æ³•è·å–æ•°æ®ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self.base_config.copy()
            
        total_samples = data_info['total_sequence_samples']
        leak_ratio = data_info['leak_ratio']
        time_span = data_info['estimated_time_span']
        
        print(f"ğŸ“Š æ•°æ®åˆ†æç»“æœ:")
        print(f"   æ€»åºåˆ—æ ·æœ¬æ•°: {total_samples:,}")
        print(f"   æ³„æ¼æ ·æœ¬æ•°: {data_info['total_leak_samples']:,}")
        print(f"   æ­£å¸¸æ ·æœ¬æ•°: {data_info['total_normal_samples']:,}")
        print(f"   æ³„æ¼æ¯”ä¾‹: {leak_ratio:.1%}")
        print(f"   æ•°æ®æ—¶é—´è·¨åº¦: {time_span:.1f} å°æ—¶")
        
        # æ ¹æ®æ•°æ®é‡çº§åˆ«ç¡®å®šé…ç½®
        if total_samples < 100_000:
            level = "å°æ•°æ®é›†"
            config = self._get_small_dataset_config()
        elif total_samples < 1_000_000:
            level = "ä¸­ç­‰æ•°æ®é›†"
            config = self._get_medium_dataset_config()
        elif total_samples < 10_000_000:
            level = "å¤§æ•°æ®é›†"
            config = self._get_large_dataset_config()
        else:
            level = "è¶…å¤§æ•°æ®é›†"
            config = self._get_xlarge_dataset_config()
            
        print(f"\nğŸ¯ æ¨èé…ç½®çº§åˆ«: {level}")
        
        return config

    def _get_small_dataset_config(self):
        """å°æ•°æ®é›†é…ç½®"""
        config = self.base_config.copy()
        config.update({
            'epochs': 150,
            'batch_size': 64,
            'learning_rate': 0.01,
            'early_stopping_patience': 20,
            'lr_decay_patience': 10,
            'training_mode': 'å¿«é€ŸéªŒè¯'
        })
        return config

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç‡ƒæ°”æ³„æ¼æ£€æµ‹ç³»ç»Ÿè®­ç»ƒé…ç½®ä¼˜åŒ–å™¨')
    parser.add_argument('--data-path', type=str, help='è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default='optimized_config.json', 
                       help='è¾“å‡ºé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--analyze-only', action='store_true', 
                       help='ä»…åˆ†ææ•°æ®ï¼Œä¸ç”Ÿæˆé…ç½®')
    
    args = parser.parse_args()
    
    print("ğŸ”§ ç‡ƒæ°”æ³„æ¼æ£€æµ‹ç³»ç»Ÿ - è®­ç»ƒé…ç½®ä¼˜åŒ–å™¨")
    print("=" * 60)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = TrainingConfigOptimizer()
    
    # åˆ†ææ•°æ®
    print("ğŸ” åˆ†æè®­ç»ƒæ•°æ®...")
    data_info = optimizer.analyze_data(args.data_path)
    
    if data_info is None:
        print("âŒ æ— æ³•åˆ†ææ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„")
        return
        
    if args.analyze_only:
        print("\nâœ… æ•°æ®åˆ†æå®Œæˆ")
        return
        
    # æ¨èé…ç½®
    print("\nğŸ¯ ç”Ÿæˆè®­ç»ƒé…ç½®å»ºè®®...")
    config = optimizer.recommend_config(data_info)
    
    print(f"\nâœ… é…ç½®ä¼˜åŒ–å®Œæˆï¼")

if __name__ == '__main__':
    main()